{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --branch VGG-16 https://github.com/LEO690201/Artificial-neural-network.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:05:10.131120Z","iopub.execute_input":"2025-03-29T05:05:10.131423Z","iopub.status.idle":"2025-03-29T05:05:10.907707Z","shell.execute_reply.started":"2025-03-29T05:05:10.131396Z","shell.execute_reply":"2025-03-29T05:05:10.906572Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Artificial-neural-network'...\nremote: Enumerating objects: 107, done.\u001b[K\nremote: Counting objects: 100% (107/107), done.\u001b[K\nremote: Compressing objects: 100% (97/97), done.\u001b[K\nremote: Total 107 (delta 42), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (107/107), 429.86 KiB | 10.23 MiB/s, done.\nResolving deltas: 100% (42/42), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/Artificial-neural-network/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:05:43.220590Z","iopub.execute_input":"2025-03-29T05:05:43.220970Z","iopub.status.idle":"2025-03-29T05:05:43.224780Z","shell.execute_reply.started":"2025-03-29T05:05:43.220942Z","shell.execute_reply":"2025-03-29T05:05:43.223923Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!python /kaggle/working/Artificial-neural-network/model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:05:58.783870Z","iopub.execute_input":"2025-03-29T05:05:58.784214Z","iopub.status.idle":"2025-03-29T05:06:05.100328Z","shell.execute_reply.started":"2025-03-29T05:05:58.784183Z","shell.execute_reply":"2025-03-29T05:06:05.099236Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]             640\n              ReLU-2         [-1, 64, 224, 224]               0\n            Conv2d-3         [-1, 64, 224, 224]          36,928\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6        [-1, 128, 112, 112]          73,856\n              ReLU-7        [-1, 128, 112, 112]               0\n            Conv2d-8        [-1, 128, 112, 112]         147,584\n              ReLU-9        [-1, 128, 112, 112]               0\n        MaxPool2d-10          [-1, 128, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]         295,168\n             ReLU-12          [-1, 256, 56, 56]               0\n           Conv2d-13          [-1, 256, 56, 56]         590,080\n             ReLU-14          [-1, 256, 56, 56]               0\n           Conv2d-15          [-1, 256, 56, 56]         590,080\n             ReLU-16          [-1, 256, 56, 56]               0\n        MaxPool2d-17          [-1, 256, 28, 28]               0\n           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n             ReLU-19          [-1, 512, 28, 28]               0\n           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n             ReLU-21          [-1, 512, 28, 28]               0\n           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n             ReLU-23          [-1, 512, 28, 28]               0\n        MaxPool2d-24          [-1, 512, 14, 14]               0\n           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n             ReLU-26          [-1, 512, 14, 14]               0\n           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n             ReLU-28          [-1, 512, 14, 14]               0\n           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n             ReLU-30          [-1, 512, 14, 14]               0\n        MaxPool2d-31            [-1, 512, 7, 7]               0\n          Flatten-32                [-1, 25088]               0\n           Linear-33                 [-1, 4096]     102,764,544\n             ReLU-34                 [-1, 4096]               0\n          Dropout-35                 [-1, 4096]               0\n           Linear-36                 [-1, 4096]      16,781,312\n             ReLU-37                 [-1, 4096]               0\n          Dropout-38                 [-1, 4096]               0\n           Linear-39                   [-1, 10]          40,970\n================================================================\nTotal params: 134,300,362\nTrainable params: 134,300,362\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.19\nForward/backward pass size (MB): 218.77\nParams size (MB): 512.32\nEstimated Total Size (MB): 731.28\n----------------------------------------------------------------\nNone\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!python /kaggle/working/Artificial-neural-network/model_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:06:13.926872Z","iopub.execute_input":"2025-03-29T05:06:13.927218Z","iopub.status.idle":"2025-03-29T05:12:42.280915Z","shell.execute_reply.started":"2025-03-29T05:06:13.927190Z","shell.execute_reply":"2025-03-29T05:12:42.279972Z"}},"outputs":[{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n100%|██████████████████████████████████████| 26.4M/26.4M [00:02<00:00, 9.71MB/s]\nExtracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n100%|███████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 208kB/s]\nExtracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n100%|██████████████████████████████████████| 4.42M/4.42M [00:01<00:00, 3.84MB/s]\nExtracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|██████████████████████████████████████| 5.15k/5.15k [00:00<00:00, 26.7MB/s]\nExtracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nEpoch 1/20\n----------\nTraceback (most recent call last):\n  File \"/kaggle/working/Artificial-neural-network/model_train.py\", line 160, in <module>\n    train_process=train_model_process(AlexNet,train_dataloader,val_dataloader,num_epochs=20)\n  File \"/kaggle/working/Artificial-neural-network/model_train.py\", line 93, in train_model_process\n    output=model(b_x)    # 前向传播,输入为一个batch,输出为一个batch中的对应预测\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/Artificial-neural-network/model.py\", line 61, in forward\n    x=self.block1(x)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 133, in forward\n    return F.relu(input, inplace=self.inplace)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1704, in relu\n    result = torch.relu(input)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 15.89 GiB of which 865.12 MiB is free. Process 3372 has 15.04 GiB memory in use. Of the allocated memory 12.93 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# %load /kaggle/working/Artificial-neural-network/model_train.py\nimport torch\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\nimport torch.utils.data as Data\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model import VGG16\nimport torch.nn as nn\nimport copy\nimport time\nimport pandas as pd\n\n'''数据加载'''\ndef train_val_data_process():   # 定义训练集和验证集的处理函数\n    train_data=FashionMNIST(root='./data',\n                            train=True,\n                            transform=transforms.Compose([transforms.Resize(size=227),transforms.ToTensor()]),\n                            download=True)\n    train_data,val_data=Data.random_split(train_data,[round(len(train_data)*0.8),   # 随机划分训练集和验证集\n                                                      round(len(train_data)*0.2)])  # 验证集\n    train_dataloader=Data.DataLoader(dataset=train_data,\n                                     batch_size=32,\n                                     shuffle=True,\n                                     num_workers=2)   # 训练集的DataLoader(划分)\n    val_dataloader=Data.DataLoader(dataset=val_data,\n                                   batch_size=32,\n                                   shuffle=False,\n                                   num_workers=2)   # 验证集的DataLoader\n    return train_dataloader,val_dataloader\n\n'''模型训练过程'''\ndef train_model_process(model,train_dataloader,val_dataloader,num_epochs):   # 定义模型训练的过程\n    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 选择设备\n    \n    # 定义优化器和损失函数\n    optimizer=torch.optim.Adam(model.parameters(),lr=0.001)  # 定义优化器，使得模型在训练过程中更新权重\n    # adam移动平均值，使得模型在训练过程中更加平滑，防止梯度爆炸，加速梯度下降\n    criterion=nn.CrossEntropyLoss()   # 定义损失函数，交叉熵   \n\n    model=model.to(device)   # 将模型加载到设备上\n\n    best_model_wts=copy.deepcopy(model.state_dict())   # 保存最佳模型参数\n\n    # 初始化参数：\n    best_acc=0.0   # 最佳准确率\n    train_loss_all=[]  # 训练集损失函数列表\n    val_loss_all=[]\n    train_acc_all=[]   # 训练集准确率列表\n    val_acc_all=[]\n    since=time.time()   # 记录训练开始时间\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1,num_epochs))   # 打印当前epoch,此时每个epoch是从1开始的，到num_epochs结束\n        print('-'*10)   # 打印分割线\n\n        # 初始化参数：\n        train_loss=0.0    # 训练集损失函数\n        train_corrects=0  # 训练的准确度\n        val_loss=0.0\n        val_corrects=0\n\n        train_num=0      #训练集样本数\n        val_num=0      #验证集样本数\n\n        '''训练阶段'''\n        for step,(b_x,b_y) in enumerate(train_dataloader):    # 遍历训练集，b_x为一个batch的输入数据，b_y为一个batch的标签数据\n        # 此时b_x为128*28*28*1的四维张量，b_y为128个标签,其中的enumerate函数返回的是一个enumerate对象，\n        # 该对象包含两个元素，第一个元素是索引，第二个元素是b_x和b_y\n            b_x=b_x.to(device)   # 将训练集的输入数据加载到设备上\n            b_y=b_y.to(device)   # 将训练集的标签数据加载到设备上\n\n            model.train()        # 开启训练模式\n            output=model(b_x)    # 前向传播,输入为一个batch,输出为一个batch中的对应预测\n\n            pre_lab=torch.argmax(output,dim=1)   # 查找每行中最大概率的行标，即预测的标签（类似softmax操作）\n            loss=criterion(output,b_y)   # 计算每个batch的损失函数\n\n            optimizer.zero_grad()   # 梯度清零初始化\n            loss.backward()         # 反向传播\n            optimizer.step()        # 更新权重,根据反向传播的梯度信息来更新网络参数，从而降低loss函数计算值的作用\n\n            train_loss+=loss.item()*b_x.size(0)   # 累加训练集的损失函数\n            train_corrects+=torch.sum(pre_lab==b_y.data)   # 如果预测正确，则累加训练集的正确数+1\n            train_num+=b_x.size(0)   # 累加训练集的样本数\n\n        '''验证阶段'''\n        for step,(b_x,b_y) in enumerate(val_dataloader):   # 遍历验证集\n            b_x=b_x.to(device)   # 将验证集的输入数据加载到设备上\n            b_y=b_y.to(device)   # 将验证集的标签数据加载到设备上\n\n            model.eval()         # 开启验证模式\n\n            output=model(b_x)    # 前向传播,输入为一个batch,输出为一个batch中的对应预测\n            pre_lab=torch.argmax(output,dim=1)   # 查找每行中最大概率的行标，即预测的标签（类似softmax操作）\n            loss=criterion(output,b_y)   # 计算每个batch的损失函数\n\n            val_loss+=loss.item()*b_x.size(0)   # 累加验证集的损失函数\n            val_corrects+=torch.sum(pre_lab==b_y.data)   # 如果预测正确，则累加验证集的正确数+1\n            val_num+=b_x.size(0)   # 累加验证集的样本数\n\n        train_loss_all.append(train_loss/train_num)   # 计算并保存每次迭代的loss值和准确率\n        val_loss_all.append(val_loss/val_num)\n        train_acc_all.append(train_corrects.double().item()/train_num)      # item()函数将tensor转换为python的float类型，\n        val_acc_all.append(val_corrects.double().item()/val_num)            # double()函数将tensor转换为python的float类型\n        \n\n        print('{}Train Loss: {:.4f}  Train Acc: {:.4f}'.format(epoch,train_loss_all[-1],train_acc_all[-1])) \n        print('{}Val Loss: {:.4f} Vac Acc: {:.4f}'.format(epoch,val_loss_all[-1],val_acc_all[-1]))\n\n        # 保存最佳模型参数,权重\n        # 寻找最佳准确度\n        if val_acc_all[-1]>best_acc:\n            best_acc=val_acc_all[-1]   # 更新最佳准确度\n            best_model_wts=copy.deepcopy(model.state_dict())     # 保存最佳模型参数,deepcopy()函数用于深度复制模型参数\n\n        # 打印训练时间\n        time_use=time.time()-since\n        print('训练耗时{:.0f}m{:.0f}s'.format(time_use//60,time_use%60))\n\n    # 选择最优参数，\n    # 加载最高准确率下的模型参数\n    model.load_state_dict(best_model_wts)\n    torch.save(model.state_dict(),'./best_model_params.pth')   # 保存模型参数\n    \n    # torch.save(best_model_wts,'./人工智能/神经网络/LeNet-5/best_model_params.pth')\n\n    train_process=pd.DataFrame(data={\n        'epoch':range(num_epochs),\n        'train_loss_all':train_loss_all,\n        'val_loss_all':val_loss_all,\n        'train_acc_all':train_acc_all,\n        'val_acc_all':val_acc_all\n    })   # 保存训练过程数据\n\n    return train_process   # 返回训练过程数据\n        \n# 根据训练过程数据绘制训练曲线\ndef matplot_acc_loss(train_process):\n    plt.figure(figsize=(12,4))  # 设置画布大小   \n    plt.subplot(1,2,1)          # 绘制训练集损失率曲线,(1,2,1)表示1行2列的第一个子图\n    plt.plot(train_process['epoch'],train_process.train_loss_all,'ro-',label='train_loss')\n    plt.plot(train_process['epoch'],train_process.val_loss_all,'bs-',label='val_loss')\n    plt.legend()      # 显示图例\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n\n    plt.subplot(1,2,2)          # 绘制训练集准确率曲线,(1,2,2)表示1行2列的第二个子图\n    plt.plot(train_process['epoch'],train_process.train_acc_all,'ro-',label='train_acc')\n    plt.plot(train_process['epoch'],train_process.val_acc_all,'bs-',label='val_acc')\n    plt.legend()      # 显示图例\n    plt.xlabel('epoch')\n    plt.ylabel('acc')\n\nif __name__=='__main__':\n    # 加载模型\n    AlexNet=VGG16()\n    # 加载数据\n    train_dataloader,val_dataloader=train_val_data_process()\n    # 训练模型\n    train_process=train_model_process(AlexNet,train_dataloader,val_dataloader,num_epochs=20)\n    # 绘制训练曲线\n    matplot_acc_loss(train_process)\n    plt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:14:08.777848Z","iopub.execute_input":"2025-03-29T05:14:08.778214Z","iopub.status.idle":"2025-03-29T05:14:08.786092Z","shell.execute_reply.started":"2025-03-29T05:14:08.778187Z","shell.execute_reply":"2025-03-29T05:14:08.785136Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/Artificial-neural-network/model_train.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!python /kaggle/working/Artificial-neural-network/model_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:14:19.124896Z","iopub.execute_input":"2025-03-29T05:14:19.125183Z","iopub.status.idle":"2025-03-29T07:37:01.585064Z","shell.execute_reply.started":"2025-03-29T05:14:19.125161Z","shell.execute_reply":"2025-03-29T07:37:01.584172Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n----------\n0Train Loss: 2.3053  Train Acc: 0.1013\n0Val Loss: 2.3028 Vac Acc: 0.0995\n训练耗时7m8s\nEpoch 2/20\n----------\n1Train Loss: 2.3028  Train Acc: 0.0972\n1Val Loss: 2.3031 Vac Acc: 0.0977\n训练耗时14m16s\nEpoch 3/20\n----------\n2Train Loss: 2.3028  Train Acc: 0.0985\n2Val Loss: 2.3030 Vac Acc: 0.0977\n训练耗时21m24s\nEpoch 4/20\n----------\n3Train Loss: 2.3028  Train Acc: 0.0965\n3Val Loss: 2.3027 Vac Acc: 0.0995\n训练耗时28m32s\nEpoch 5/20\n----------\n4Train Loss: 2.3028  Train Acc: 0.1002\n4Val Loss: 2.3028 Vac Acc: 0.0963\n训练耗时35m39s\nEpoch 6/20\n----------\n5Train Loss: 2.3027  Train Acc: 0.1001\n5Val Loss: 2.3029 Vac Acc: 0.0963\n训练耗时42m47s\nEpoch 7/20\n----------\n6Train Loss: 2.3028  Train Acc: 0.0989\n6Val Loss: 2.3030 Vac Acc: 0.0963\n训练耗时49m54s\nEpoch 8/20\n----------\n7Train Loss: 2.3028  Train Acc: 0.0985\n7Val Loss: 2.3028 Vac Acc: 0.0977\n训练耗时57m2s\nEpoch 9/20\n----------\n8Train Loss: 2.3028  Train Acc: 0.0991\n8Val Loss: 2.3030 Vac Acc: 0.0963\n训练耗时64m9s\nEpoch 10/20\n----------\n9Train Loss: 2.3027  Train Acc: 0.0980\n9Val Loss: 2.3028 Vac Acc: 0.0971\n训练耗时71m17s\nEpoch 11/20\n----------\n10Train Loss: 2.3028  Train Acc: 0.0983\n10Val Loss: 2.3027 Vac Acc: 0.1037\n训练耗时78m24s\nEpoch 12/20\n----------\n11Train Loss: 2.3028  Train Acc: 0.0976\n11Val Loss: 2.3028 Vac Acc: 0.0977\n训练耗时85m32s\nEpoch 13/20\n----------\n12Train Loss: 2.3028  Train Acc: 0.0986\n12Val Loss: 2.3028 Vac Acc: 0.0995\n训练耗时92m40s\nEpoch 14/20\n----------\n13Train Loss: 2.3027  Train Acc: 0.0989\n13Val Loss: 2.3027 Vac Acc: 0.0971\n训练耗时99m48s\nEpoch 15/20\n----------\n14Train Loss: 2.3028  Train Acc: 0.0977\n14Val Loss: 2.3028 Vac Acc: 0.0963\n训练耗时106m56s\nEpoch 16/20\n----------\n15Train Loss: 2.3028  Train Acc: 0.0996\n15Val Loss: 2.3030 Vac Acc: 0.0971\n训练耗时114m4s\nEpoch 17/20\n----------\n16Train Loss: 2.3028  Train Acc: 0.0981\n16Val Loss: 2.3028 Vac Acc: 0.0963\n训练耗时121m12s\nEpoch 18/20\n----------\n17Train Loss: 2.3027  Train Acc: 0.0987\n17Val Loss: 2.3026 Vac Acc: 0.1003\n训练耗时128m20s\nEpoch 19/20\n----------\n18Train Loss: 2.3028  Train Acc: 0.0973\n18Val Loss: 2.3027 Vac Acc: 0.0992\n训练耗时135m27s\nEpoch 20/20\n----------\n19Train Loss: 2.3027  Train Acc: 0.0986\n19Val Loss: 2.3029 Vac Acc: 0.1003\n训练耗时142m35s\nFigure(1200x400)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}